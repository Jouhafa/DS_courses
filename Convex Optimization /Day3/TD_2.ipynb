{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Proximal gradient descent\n",
    "Author : Pierre Ablin\n",
    "\n",
    "In this lab, the goal is to implement proximal gradient descent, and look at its behavior. We will implement it for the Lasso, and some $\\ell_1$ regularized problems.\n",
    "\n",
    "\n",
    "In the following, `X` is a $n\\times p$ matrix and $y$ a vector of size `n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 5\n",
    "p = 10\n",
    "X = np.random.randn(n, p)\n",
    "y = np.random.randn(n)\n",
    "w = np.random.randn(p)\n",
    "lbda = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1**: Implement an algorithm that computes the value of the lasso function, and of the gradient of the smooth part. We recall that the cost function is\n",
    "\n",
    "$$\\min_w \\frac12\\|Xw - y\\|^2 + \\lambda\\|w\\|_1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_cost(w, X, y, lbda):\n",
    "    # your code here\n",
    "    return cost\n",
    "\n",
    "\n",
    "def lasso_gradient(w, X, y):\n",
    "    # your code here\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2** Implement the proximal operator of the $\\ell_1$ norm, a.k.a. the soft thresholding operator:\n",
    "\n",
    "$$\\mathrm{ST}(x, u) = \\begin{cases}x - u \\text{ if } x > u\\\\ 0 \\text{ if } -u \\leq x \\leq u \\\\ x+ u \\text{ else}\\end{cases}$$\n",
    "\n",
    "It should be vectorized, so that when $x$ is a vector it takes the soft-threshloding of each coordinate.\n",
    "Plot it in 1-D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_thresholding(x, u):\n",
    "    # your code here\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-2, 2)\n",
    "plt.plot(x, soft_thresholding(x, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3** Demonstrate that if $\\lambda \\geq \\lambda_{\\max} \\triangleq \\max_{i=1, \\dots, p} |[X^{\\top}y]_i|$ then $0$ is a solution of the Lasso. \n",
    "\n",
    "\n",
    "As a consequence, we take for $\\lambda$ a fraction of $\\lambda_{\\max}$, for instance $\\lambda = 0.5 \\lambda_{\\max}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_max = np.max(np.abs(X.T.dot(y)))\n",
    "lbda = 0.5 * lambda_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4** Implement proximal gradient descent with a step size `step` for `n_iters` iterations, starting from $w^0=0$.  Display the evolution of $f(x_n) - f^*$ for this problem, with 100 iterations and the classical step size $1/L$. You can compute $f^*$ by running the algorithm for many iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proximal_gradient_descent(X, y, lbda, step, n_iters):\n",
    "    n, p = X.shape\n",
    "    w = np.zeros(p)\n",
    "    w_list = []\n",
    "    for i in range(n_iters):\n",
    "        w_list.append(w.copy())\n",
    "        # your code here\n",
    "    return w, w_list  # returns the value of w and the list of iterates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step =  # your code here\n",
    "n_iters = 100\n",
    "w, w_list = proximal_gradient_descent(X, y, lbda, step, n_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_vals = np.array([lasso_cost(w, X, y, lbda) for w in w_list])\n",
    "f_star = np.min(f_vals)\n",
    "\n",
    "\n",
    "plt.semilogy(f_vals - f_star)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('f - f^*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_star = w_list[-1]\n",
    "dists = [np.linalg.norm(w - w_star) for w in w_list]\n",
    "\n",
    "plt.semilogy(dists)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('||x - x^*||')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5** what do you notice about the optimal $w$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 6** Generate a problem with $n=50$, $p=100$ where the conditioning is $~1 / 100$. For various values of $\\lambda$, what convergence curves do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, p = 50, 100\n",
    "\n",
    "y = np.random.randn(n)\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different regularizations\n",
    "\n",
    "We now consider $A$ a $p \\times p$ matrix, and consider the problem\n",
    "$$\n",
    "\\min_{w} \\frac12\\|Xw - y\\|^2 + \\lambda \\|Aw\\|_1\n",
    "$$\n",
    "\n",
    "In order to implement the proximal gradient descent, we need to compute the proximal operator of the second term.\n",
    "\n",
    "We define $R(w) = \\lambda\\|Aw\\|_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 7** Assume that $A$ is diagonal with coefficients $a_1, \\dots, a_p$. What is $\\mathrm{prox}_R(w)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 8** Assume that $A$ is orthogonal, i.e. $AA^{\\top} = I_p$. What is $\\mathrm{prox}_R(w)$?\n",
    "\n",
    "Hint: Let $\\min_{x} F(x, w)= \\frac12\\|x - w\\|^2 + \\lambda\\|Ax\\|_1$. What does the change of variable $z = Ax$ give?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 9** In general, when $A$ is neither diagonal nor orthogonal, we do not have a closed-form expression of the proximal operator of $R$. \n",
    "\n",
    "We can instead resort once again to an iterative algorithm to compute it.\n",
    "\n",
    "\n",
    "Prove that $\\mathrm{prox}_{\\lambda R(w)} = A^{-1} \\phi(w, A^{-1}, \\lambda)$ where $\\phi(w, B, \\lambda) = \\arg\\min_z \\frac12\\|Bz - w\\| +\\lambda \\|z\\|_1$\n",
    "\n",
    "What problem do you recognize? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
